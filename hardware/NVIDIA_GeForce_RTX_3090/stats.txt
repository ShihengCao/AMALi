running ./MaxFlops_double microbenchmark
DPU FLOP per SM = 1.288979 (flop/clk/SM)
Total Clk number = 26031798 
/////////////////////////////////
running ./MaxFlops_float microbenchmark
FLOP per SM = 126.180534 (flop/clk/SM)
Total Clk number = 66481 
/////////////////////////////////
running ./MaxFlops_half microbenchmark
half FLOP per SM = 247.978241 (flop/clk/SM)
Total Clk number = 16914 
/////////////////////////////////
running ./MaxFlops_int32 microbenchmark
int32 FLOP per SM = 126.454437 (flop/clk/SM)
Total Clk number = 66337 
/////////////////////////////////
running ./atomic_add_bw microbenchmark
Atomic int32 bandwidth = 0.000127 (byte/clk)
Total Clk number = 43246319133381 
/////////////////////////////////
running ./atomic_add_bw_conflict microbenchmark
Atomic int32 bandwidth = 0.117400 (byte/clk)
Total Clk number = 2929586703 
/////////////////////////////////
running ./atomic_add_lat microbenchmark
Atomic int32 latency = 270.347656 (clk)
Total Clk number = 276836 
/////////////////////////////////
running ./config_dpu microbenchmark
DPU FLOP per SM = 1.289171 (flop/clk/SM)
Total Clk number = 26027909 
double-precision DPU latency = 55.027527 (clk)
Total Clk number = 901571 

//Accel_Sim config: 
-gpgpu_num_dp_units 4
-ptx_opcode_latency_dp 55,55,55,55,330
-ptx_opcode_initiation_dp 128,128,128,128,130
-trace_opcode_latency_initiation_dp 55,128
/////////////////////////////////
running ./config_fpu microbenchmark
FLOP per SM = 126.197617 (flop/clk/SM)
Total Clk number = 66472 
float-precision FPU latency = 4.070740 (clk)
Total Clk number = 66695 

//Accel_Sim config: 
-gpgpu_num_sp_units 4
-ptx_opcode_latency_fp 4,4,4,4,39
-ptx_opcode_initiation_fp 2,2,2,2,4
-trace_opcode_latency_initiation_sp 4,2
/////////////////////////////////
running ./config_int microbenchmark
int32 FLOP per SM = 126.609032 (flop/clk/SM)
Total Clk number = 66256 
int32 latency = 4.166260 (clk)
Total Clk number = 17065 

//Accel_Sim config: 
-gpgpu_num_int_units 4
-ptx_opcode_latency_int 4,4,4,4,21
-ptx_opcode_initiation_int 2,2,2,2,2
-trace_opcode_latency_initiation_int 4,2
/////////////////////////////////
running ./config_sfu microbenchmark
SFU fast sqrt bw = 15.9734(flops/clk/SM) 
Total Clk number = 262580
SFU fast sqrt latency = 23.1387(clk) 
Total Clk number = 94776

//Accel_Sim config: 
-gpgpu_num_sfu_units 4
-ptx_opcode_latency_sfu 23
-ptx_opcode_initiation_sfu 8
-trace_opcode_latency_initiation_sfu 23,8
/////////////////////////////////
running ./config_tensor microbenchmark
wmma PTX issue bandwidth = 1.99866(thread/clk/SM) 
hmma SASS issue bandwidth = 31.9785(thread/clk/SM)
FMA tensor bandwidth = 255.828(FMA/clk/SM)
Total Clk number = 1049280
wmma latency = 64.1431(clk)
hmma latency = 4.00894(clk)
Total Clk number = 262730

//Accel_Sim config: 
-gpgpu_tensor_core_avail 1
-gpgpu_num_tensor_core_units 4
-ptx_opcode_latency_tesnor 64
-ptx_opcode_initiation_tensor 64
-trace_opcode_latency_initiation_tensor 4,4
-specialized_unit_3 1,4,4,4,4,TENSOR
-trace_opcode_latency_initiation_spec_op_3 4,4
/////////////////////////////////
running ./config_udp microbenchmark
-specialized_unit_4 1,4,4,4,4,UDP
-trace_opcode_latency_initiation_spec_op_4 4,1
/////////////////////////////////
running ./core_config microbenchmark
CUDA version number = 8.6

//Accel_Sim config: 
-gpgpu_ptx_force_max_capability 86
-gpgpu_shader_registers 65536
-gpgpu_registers_per_block 65536
-gpgpu_occupancy_sm_number 86
-gpgpu_coalesce_arch 86
-gpgpu_pipeline_widths 4,4,4,4,4,4,4,4,4,4,8,4,4
-gpgpu_sub_core_model 1
-gpgpu_enable_specialized_operand_collector 0
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8
-gpgpu_num_sched_per_core 4
-gpgpu_max_insn_issue_per_warp 1
-gpgpu_dual_issue_diff_exec_units 1
-gpgpu_inst_fetch_throughput 4
-gpgpu_shader_core_pipeline 1536:32
-gpgpu_shader_cta 32
/////////////////////////////////
running ./deviceQuery microbenchmark
  Device : "NVIDIA GeForce RTX 3090"

  CUDA version number                         : 8.6
  GPU Max Clock rate                             : 1695 MHz 
  Multiprocessors Count                       : 82
  Maximum number of threads per multiprocessor: 1536
  CUDA Cores per multiprocessor               : 128 
  Registers per multiprocessor                : 65536
  Shared memory per multiprocessor            : 102400 bytes
  Warp size                                   : 32
  Maximum number of threads per block         : 1024
  Shared memory per block                     : 49152 bytes
  Registers per block                         : 65536
  globalL1CacheSupported                      : 1
  localL1CacheSupported                       : 1
  L2 Cache Size                             : 6 MB
  Global memory size                        : 24 GB
  Memory Clock rate                           : 9751 Mhz
  Memory Bus Width                            : 384 bit
 ////////////////////////// 
/////////////////////////////////
running ./kernel_lat microbenchmark
Kernel Launch Latency = 3686.4 cycles
The reported latency above can be slightly higher than real. For accurate evaultion using nvprof event, exmaple: make events ./kernel_lat

//Accel_Sim config: 
-gpgpu_kernel_launch_latency  3686
/////////////////////////////////
running ./l1_access_grain microbenchmark

This benchmark measures coalescing granularity for differnet strides.
check the nvprof or nvsight for received l1 reads and writes.
to run the program with nsight: make nvsight ./l1_access_grain
stats to look at: l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum

/////////////////////////////////
running ./l1_adaptive microbenchmark
The ubench is not imepleneted yet.
/////////////////////////////////
running ./l1_associativity microbenchmark
Launching L1 cache line size ubench
Saving L1 cache line size data at L1line.csv
Launching L1 cache assoc ubench
Saving L1 cache assoc data at L1asso.csv
/////////////////////////////////
running ./l1_banks microbenchmark
The ubench is not imepleneted yet.
/////////////////////////////////
running ./l1_bw_128 microbenchmark
L1 bandwidth = 120.084(byte/clk/SM), 134.205(GB/s/SM)
Total Clk number = 34928
/////////////////////////////////
running ./l1_bw_32f microbenchmark
L1 bandwidth = 63.2596(byte/clk/SM), 70.6982(GB/s/SM)
Total Clk number = 66303
/////////////////////////////////
running ./l1_bw_32f_unroll microbenchmark
L1 bandwidth = 56.087830 (byte/clk/SM)
Total Clk number = 74781 
/////////////////////////////////
running ./l1_bw_64f microbenchmark
L1 bandwidth = 13.4458(byte/clk/SM), 15.0269(GB/s/SM)
Total Clk number = 311941
/////////////////////////////////
running ./l1_bw_64v microbenchmark
L1 bandwidth = 114.361(byte/clk/SM), 127.808(GB/s/SM)
Total Clk number = 18338
/////////////////////////////////
running ./l1_config microbenchmark

//Accel_Sim config: 
-gpgpu_adaptive_cache_config 1
-gpgpu_shmem_option 0,8,16,32,64,100
-gpgpu_unified_l1d_size 128
-gpgpu_l1_banks 4
-gpgpu_cache:dl1 S:4:128:64,L:T:m:L:L,A:384:48,16:0,32
-gpgpu_gmem_skip_L1D 0
-gpgpu_l1_cache_write_ratio 25
/////////////////////////////////
running ./l1_lat microbenchmark
L1 Latency  =      41.9189 cycles
Total Clk number = 1373599 

//Accel_Sim config: 
-gpgpu_l1_latency 41
/////////////////////////////////
running ./l1_mshr microbenchmark
Launching L1 MSHR ubench
Saving L1 MSHR data at MSHR100_array1073741824_shmem12288_itr6.csv
/////////////////////////////////
running ./l1_sector microbenchmark
Launching L1 sector ubench
Saving L1 sector data at data.csv
/////////////////////////////////
running ./l1_shared_bw microbenchmark
Shared Memory Bandwidth = 62.310229 (byte/clk/SM)
Total Clk number = 538506 
/////////////////////////////////
running ./l1_write_policy microbenchmark

This microbenchmark detects L1 write policy.
check the nvprof or nvsight for received l1 reads and writes to detect the policy.
see the code comments for further details
to run the program with nvsight: make nvsight ./l1_write_policy
stats to look at: l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_hit.sum & l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_hit.sum 

/////////////////////////////////
running ./l2_access_grain microbenchmark

This benchmark measures l2 access granularity for differnet strides.
check the nvprof or nvsight for received l2 reads and write.
to run the program with nsight: make nvsight ./l2_access_grain
stats to look at: lts__t_sectors_srcunit_tex_op_read.sum and lts__t_sectors_srcunit_tex_op_write.sum 

/////////////////////////////////
running ./l2_bw_128 microbenchmark
L2 bandwidth = 1386.32(byte/clk), 1549.34(GB/s)
Max Theortical L2 bandwidth = 192(byte/clk), 214.577(GB/s)
L2 BW achievable = 722.043%
Total Clk number = 248090
/////////////////////////////////
running ./l2_bw_32f microbenchmark
L2 bandwidth = 1259.25(byte/clk), 1407.32(GB/s)
Max Theortical L2 bandwidth = 192(byte/clk), 214.577(GB/s)
L2 BW achievable = 655.857%
Total Clk number = 546252
/////////////////////////////////
running ./l2_bw_64f microbenchmark
L2 bandwidth = 1095.94(byte/clk), 1224.8(GB/s)
Max Theortical L2 bandwidth = 192(byte/clk), 214.577(GB/s)
L2 BW achievable = 570.799%
Total Clk number = 1255304
/////////////////////////////////
running ./l2_config microbenchmark
L2 Cache Size = 6 MB
L2 Banks number = 6

//Accel_Sim config: 
-gpgpu_n_sub_partition_per_mchannel 2
-icnt_flit_size 40
-gpgpu_memory_partition_indexing 0
-gpgpu_cache:dl2 S:512:128:16,L:B:m:L:X,A:192:4,32:0,32
/////////////////////////////////
running ./l2_copy_engine microbenchmark
L2 Latency no-warmp up =     853.1735 cycles 
Total Clk number = 27956788 
L2 Hit Latency =     796.2650 cycles 
Total Clk number = 26092012 
Is memcpy cached in L2? Yes, error=7.2

//Accel_Sim config: 
-gpgpu_perf_sim_memcpy 1
/////////////////////////////////
running ./l2_lat microbenchmark
L2 Hit Latency =     798.2529 cycles 
Total Clk number = 26157150 
L1 Latency  =      41.9268 cycles
Total Clk number = 1373856 

//Accel_Sim config: 
-gpgpu_l2_rop_latency 756
/////////////////////////////////
running ./l2_write_policy microbenchmark

This microbenchmark detects L2 write policy.
check the nvprof or nvsight for received L2 reads and writes to detect the policy.
see the code comments for further details
to run the program with nvsight: make nvsight ./2
stats to look at: llts__t_sectors_srcunit_tex_op_read.sum & lts__t_sectors_srcunit_tex_op_write.sum & lts__t_sectors_srcunit_tex_op_read_lookup_hit.sum & lts__t_sectors_srcunit_tex_op_write_lookup_hit.sum 

/////////////////////////////////
running ./lat_double microbenchmark
double-precision DPU latency = 55.039978 (clk)
Total Clk number = 901775 
/////////////////////////////////
running ./lat_float microbenchmark
float-precision FPU latency = 4.081177 (clk)
Total Clk number = 66866 
/////////////////////////////////
running ./lat_half microbenchmark
fpu16 latency = 4.261475 (clk)
Total Clk number = 17455 
/////////////////////////////////
running ./lat_int32 microbenchmark
int32 latency = 4.211670 (clk)
Total Clk number = 17251 
/////////////////////////////////
running ./list_devices microbenchmark

Device 0: "NVIDIA GeForce RTX 3090 sm_8.6"
/////////////////////////////////
running ./mem_atom_size microbenchmark

This benchmark measures mem atom size granularity
check the nvprof or nvsight for received mem reads and writes
to run the program with nsight: make nvsight ./l2_access_grain
stats to look at: dram__sectors_read.sum & dram__sectors_write.sum & dram__bytes_read.sum & dram__sectors_read.sum

we launched 3145728 read memory reqs (1 req per thread) with a stride of 32 (128 bytes)
if the number of memory reads is the same as read reqs, then mem atom size is 32B
if the number of memory reads is 2X issued read reqs, then mem atom size is 64B, etc.

/////////////////////////////////
running ./mem_bw microbenchmark
Mem BW= 445.723114 (Byte/Clk)
Mem BW= 28.434160 (GB/sec)
Max Theortical Mem BW= 936.096008 (GB/sec)
Mem Efficiency = 47.615108 %
Total Clk number = 169382 
/////////////////////////////////
running ./mem_config microbenchmark
Global memory size = 24 GB
Memory Clock rate = 9751 Mhz
Memory Bus Width = 384 bit
Memory type = HBM
Memory channels = 3

//Accel_Sim config: 
-gpgpu_n_mem 3
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 16
-gpgpu_dram_burst_length 2
-dram_data_command_freq_ratio 2
-dram_dual_bus_interface 1
-gpgpu_dram_timing_opt nbk=16:CCD=1:RRD=40:RCD=137:RAS=322:RP=137:RC=459:CL=137:WL=20:CDLR=30:WR=118:nbkgrp=4:CCDL=20:RTPL=40
/////////////////////////////////
running ./mem_lat microbenchmark
Mem latency =    1590.6259 cycles 
Total Clk number = 13030407 
L2 Hit Latency =     798.4058 cycles 
Total Clk number = 26162160 

//Accel_Sim config: 
-dram_latency 792
/////////////////////////////////
running ./regfile_bw microbenchmark
wmma PTX issue bandwidth = 1.99866(thread/clk/SM) 
hmma SASS issue bandwidth = 31.9786(thread/clk/SM)
FMA tensor bandwidth = 255.829(FMA/clk/SM)
Total Clk number = 1049277

regfile_bw = 1024 (byte/SM)

//Accel_Sim config: 
-gpgpu_num_reg_banks 8
-gpgpu_reg_file_port_throughput 2
/////////////////////////////////
running ./sfu_bw_fsqrt microbenchmark
SFU fast sqrt bw = 15.9682(flops/clk/SM) 
Total Clk number = 262666
/////////////////////////////////
running ./sfu_lat_fsqrt microbenchmark
SFU fast sqrt latency = 23.1687(clk) 
Total Clk number = 94899
/////////////////////////////////
running ./shared_bw microbenchmark
Shared Memory Bandwidth = 63.8563(byte/clk/SM), 71.365(GB/s/SM)
Total Clk number = 262734
/////////////////////////////////
running ./shared_bw_64 microbenchmark
Shared Memory Bandwidth = 127.901(byte/clk/SM), 142.94(GB/s/SM)
Total Clk number = 262347
/////////////////////////////////
running ./shared_lat microbenchmark
Shared Memory Latency  = 29.033203 cycles
Total Clk number = 59460 

//Accel_Sim config: 
-gpgpu_smem_latency 29
/////////////////////////////////
running ./shd_config microbenchmark
Shared memory per multiprocessor = 102400 bytes
Shared memory per block = 49152 bytes

//Accel_Sim config: 
-gpgpu_shmem_size 102400
-gpgpu_shmem_sizeDefault 102400
-gpgpu_shmem_per_block 49152
/////////////////////////////////
running ./system_config microbenchmark
Device Name = NVIDIA GeForce RTX 3090
GPU Max Clock rate = 1695 MHz 
GPU Base Clock rate = 1200 MHz 
SM Count = 82
CUDA version number = 8.6

//Accel_Sim config: 
-gpgpu_compute_capability_major 8
-gpgpu_compute_capability_minor 6
-gpgpu_n_clusters 82
-gpgpu_n_cores_per_cluster 1
-gpgpu_clock_domains 1200:1200:1200:9751
/////////////////////////////////
running ./tensor_bw_half microbenchmark
FP16 operand, FP32 accumalte:
wmma PTX issue bandwidth = 1.99866(thread/clk/SM) 
hmma SASS issue bandwidth = 31.9786(thread/clk/SM)
FMA tensor bandwidth = 255.829(FMA/clk/SM)
Total Clk number = 1049278

FP16 operand, FP16 accumalte:
wmma PTX issue bandwidth = 3.99588(thread/clk/SM) 
hmma SASS issue bandwidth = 63.934(thread/clk/SM)
FMA tensor bandwidth = 511.472(FMA/clk/SM)
Total Clk number = 524829
/////////////////////////////////
running ./tensor_lat_half microbenchmark
FP16 operand, FP32 accumalte:
wmma latency = 64.1663(clk)
hmma latency = 4.01039(clk)
Total Clk number = 262825

FP16 operand, FP16 accumalte:
wmma latency = 33.105(clk)
hmma latency = 2.06906(clk)
Total Clk number = 135598
/////////////////////////////////
